data:
  organised_data_folder_path: "data/data_sources"
  shakespeare_data_source: "dataset_shakespeare_collection.json"
  impostors_data_source: "dataset_impostors.json"
  all_impostors_data_source: "dataset_all_impostors.json"
  classify_text_data_source: "text_to_classify.json"
  trained_models_path: "saved_trained_models/"
  shakespeare_path: "data/raw/shakespeare"
  impostors_path: "data/raw/impostors"
  fine_tuned_bert_model_path: "saved_trained_models/bert/bert-shakespeare-mlm"

training:
  optimizer:
    initial_learning_rate: 1e-4
    learning_rate_decay_factor: 0.1
    gradient_clipping_threshold: 1.0 # or 5.0
#  batch_factor: 100 # Chunk to Batch ratio
#  chunk_size: 400 # 32 # or 64
  batch_size: 8
  hidden_size: 768
  epochs: 10 # 10-20
  early_stopping_patience: 3
  impostor_chunk_ratio: 2
  test_split: 0.25

model:
  save_trained_models: True
  load_trained_models: True
  cnn:
    filters: 128 # 128 or 256
    padding: "same"
    kernel_size: [3, 5, 7]
    pool_size: 2
  bilstm:
    dropout: 0.1 # 0.2 - 0.5
    number_of_layers: 50
    units: 128
  fc:
    in_features: 128
    out_features: 1

isolation_forest:
  number_of_trees: 100

bert:
  model: "bert-base-uncased"
  repository: "ElyMK1/bert-shakespeare-english-mlm"
  maximum_sequence_length: 128
  trainable: True
  mlm_probability: 0.15
  train_batch_size: 16
  num_epochs: 5
  save_steps: 1000
  save_total_limit: 2
  logging_steps: 500
  logging_dir: "logs/shakespeare"
  token: "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"


wandb:
  enabled: False
  run_name: "full-procedure-run"
  project: "siamese-authorship-verification"
  api_key: 99cb11ef59cac847e6b99323c0d485a69157f548