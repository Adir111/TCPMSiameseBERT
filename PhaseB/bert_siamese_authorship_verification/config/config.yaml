data:
  train_path: "../data/processed/dataset.json"
  test_path: "../data/processed/test.json"
  val_path: "../data/processed/val.json"
  model_path: "models/bert_siamese.pth"
  shakespeare_path: "data/raw/shakespeare"
  impostors_path: "data/raw/impostors"


training:
  optimizer:
    type: "AdamW"
    initial_learning_rate: 1e-4
    learning_rate_decay_factor: 0.1
    gradient_clipping_threshold: 1.0 # or 5.0
  batch_size: 2 # 32 # or 64
  epochs: 1 # 10-20 # VVVVVV
  early_stopping_patience: 5 # VVVVVV

model:
  hidden_size: 768 # VVVVVVVV
  softmax_dim: 1 # VVVVVV
  bidirectional: True # VVVVVV

  cnn:
    filters: 2 # 256 # VVVVVV
    stride: 1 # VVVVVV
    padding: "same" # VVVVVV
    kernel_size: 3 # or 5 or 7 # VVVVVV

  bilstm:
    hidden_units: 16 # 128 # or 256 # vvvvvv
    dropout: 0.5 # 0.2 - 0.5 # VVVVVV
    number_of_layers: 300 # vvvvvv

  fc:
    in_features: 128 # VVVVVV
    out_features: 1 # VVVVVV

isolation_forest:
  number_of_trees: 100 # VVVVVV

bert:
  model: "bert-base-uncased" # VVVVVV
  maximum_sequence_length: 64 # 512 # tokens # VVVVVV
  batch_size_for_fine_tuning: 16 # or 32
  learning_rate: 2e-5 # VVVVVV