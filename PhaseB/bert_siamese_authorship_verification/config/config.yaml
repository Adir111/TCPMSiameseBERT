data:
  processed_tested_path: "data/processed/dataset_tested_collection.json"
  processed_impostors_path: "data/processed/dataset_impostors.json"
  trained_models_path: "models/saved_trained_models/"
  shakespeare_path: "data/raw/shakespeare"
  impostors_path: "data/raw/impostors"

training:
  optimizer:
    type: "AdamW"
    initial_learning_rate: 1e-4
    learning_rate_decay_factor: 0.1
    gradient_clipping_threshold: 1.0 # or 5.0
  batch_size: 400 # 32 # or 64 - How many chunks in a batch
  chunk_factor: 8 # Chunk to Batch ratio
  epochs: 1 # 10-20
  early_stopping_patience: 2
  impostor_chunk_ratio: 2

model:
  save_trained_models: True
  load_trained_models: True
  softmax_dim: 1 # Unused
  cnn:
    filters: 2 # 256
    stride: 1
    padding: "valid"
    kernel_size: [3, 6, 12]
  bilstm:
    hidden_units: 16 # 128 or 256
    dropout: 0.5 # 0.2 - 0.5
    number_of_layers: 300
  fc:
    in_features: 128 # Unused
    out_features: 1

isolation_forest:
  number_of_trees: 100

bert:
  model: "bert-base-uncased"
  maximum_sequence_length: 64 # 512 tokens
  trainable: True
  batch_size_for_fine_tuning: 16 # or 32 - Unused
  learning_rate: 2e-5 # Unused

wandb:
  enabled: true
  project: "siamese-authorship-verification"
  api_key: 99cb11ef59cac847e6b99323c0d485a69157f548